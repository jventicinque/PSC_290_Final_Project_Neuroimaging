##Thresholding Walkthrough

#Load in Packages
from nilearn import plotting
import pylab as plt
%matplotlib inline
import numpy as np
from nilearn import image
import nibabel as nib
from nilearn.input_data import NiftiMasker
from __future__ import print_function, division
import scipy.stats as sst

#Load in data
unthresh_data = nib.load('zstat4.nii.gz')
print(unthresh_data)

##Let's visualize the data first and then we will run some different thresholding techniques on it
#plot data
plotting.plot_roi(unthresh_data,
                 cmap='Paired', title='Unthreshholded Data', draw_cross=False)

##First, we need to make a mask of the whole brain and determine the number of voxels in data (this will be count as the "number of tests" that are being run).

#Make a mask of only voxels in the brain
from nilearn.masking import compute_epi_mask
mask_img = compute_epi_mask(unthresh_data)

#load in a mask of the mPFC
mask_file = 'mPFC_Bilateral_-8_54_24.nii'

#Visualize mask on thresholded data and MNI template
plotting.plot_roi(mask_file, bg_img= unthresh_data,
                 cmap='Paired', title='ROI of mPFC', draw_cross=False)

# create the mask to use and standardize
masker = NiftiMasker(mask_img=mask_file, standardize=True)
# fit the mask
thresh_masked = masker.fit_transform(unthresh_data)

#Determine the shape of the mask to see how many "independent tests" are being run
thresh_masked.shape

##This tells us that 515 voxels are in the current mask, so we use this as if we have 515 "independent tests"
#Assign the number of voxels in the mask to "n"
n = thresh_masked.shape[1]
print(n)
#Assign an alpha level of .05
alpha = .05

##Bonferonni Correction
#Is this test too conservative for neuroimaging data?
#bonferonni = alpha/ number of voxels 

def bonferroni_thresh(alpha, n):
    return alpha / n
print(bonferroni_thresh (.05, 515))

## As we can see, a p-value of .000097... is not reasonable to maintain significance, 
##implying that this test is much too strict to correct for multiple comparisons using neuroimaging data

#plot to demonstrate bonferonni 



##Family Wise Error Rate
#Is each test independent? Likely not, as there is some degree of spatial correlation
#Calculate FWER
#plot FWER

##False Discovery Rate
## FDR controls for the number of false positives as a proportion of the number of all tests declared significant.
## This is instead of controlling for the risk of any false positives

#Assuming we have 515 "tests" as determined from our earlier steps, let's creat a distribution of scores derived from 515 tests
n = 515
z_values = np.random.normal(size=n)

#Turn z scores into p-values
normal_distribution = sst.norm(loc=0,scale=1.)
p_values = normal_distribution.cdf(z_values)

#Sort the p values from smallest to largest
p_values = np.sort(p_values)
i = np.arange(1, n+1)

#Plot a figure that shows a distribution of the number of false positives likely with 515 tests
plt.plot(i, p_values, '.')
[...]
plt.ylabel('p value')
#Put a red line at p = .05 to show the threshold of how many tests fall below that
alpha =.05
plt.plot(i, alpha * i / n, 'r', label='$q i / N$')

##It looks like only a very small number of tests near the beginning of the distribution fall under this threshold
